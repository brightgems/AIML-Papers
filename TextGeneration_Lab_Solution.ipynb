{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QambAoV-BNmI"
   },
   "source": [
    "## Text Generation Using patent abstracts from patent search for `neural network`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FM4VYS13uUq9"
   },
   "source": [
    "### Files required:\n",
    "\n",
    "1. `neural_network_patent_query.csv`\n",
    "\n",
    "2. `train-embeddings-rnn.h5`\n",
    "\n",
    "Download from [this](https://drive.google.com/drive/folders/1cbAesB-eejsRKdCHpnFSyXiu81Y5a5HU?usp=sharing) link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0s5ciFSXzeWb"
   },
   "source": [
    "### Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SiaGSpFezFzT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bC-EKMBpzjP_"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "-TwLrr9yzjSN",
    "outputId": "692612f1-b60e-434d-d2ff-05d51da48c88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RUqqTXXAzuWR"
   },
   "outputs": [],
   "source": [
    "project_path = \"/content/drive/My Drive/AIML_Labs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ExWvNjqE1EYt"
   },
   "outputs": [],
   "source": [
    "dataset = project_path + 'neural_network_patent_query.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "BCheYc6azdFl",
    "outputId": "5ef79780-859d-4a0d-b527-f8112979b7e1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patent_abstract</th>\n",
       "      <th>patent_date</th>\n",
       "      <th>patent_number</th>\n",
       "      <th>patent_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\" A \"\"Barometer\"\" Neuron enhances stability in...</td>\n",
       "      <td>1996-07-09</td>\n",
       "      <td>5535303</td>\n",
       "      <td>\"\"\"Barometer\"\" neuron for a neural network\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\" This invention is a novel high-speed neural ...</td>\n",
       "      <td>1993-10-19</td>\n",
       "      <td>5255349</td>\n",
       "      <td>\"Electronic neural network for solving \"\"trave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An optical information processor for use as a ...</td>\n",
       "      <td>1995-01-17</td>\n",
       "      <td>5383042</td>\n",
       "      <td>3 layer liquid crystal neural network with out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A method and system for intelligent control of...</td>\n",
       "      <td>2001-01-02</td>\n",
       "      <td>6169981</td>\n",
       "      <td>3-brain architecture for an intelligent decisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A method and system for intelligent control of...</td>\n",
       "      <td>2003-06-17</td>\n",
       "      <td>6581048</td>\n",
       "      <td>3-brain architecture for an intelligent decisi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     patent_abstract patent_date  \\\n",
       "0  \" A \"\"Barometer\"\" Neuron enhances stability in...  1996-07-09   \n",
       "1  \" This invention is a novel high-speed neural ...  1993-10-19   \n",
       "2  An optical information processor for use as a ...  1995-01-17   \n",
       "3  A method and system for intelligent control of...  2001-01-02   \n",
       "4  A method and system for intelligent control of...  2003-06-17   \n",
       "\n",
       "  patent_number                                       patent_title  \n",
       "0       5535303        \"\"\"Barometer\"\" neuron for a neural network\"  \n",
       "1       5255349  \"Electronic neural network for solving \"\"trave...  \n",
       "2       5383042  3 layer liquid crystal neural network with out...  \n",
       "3       6169981  3-brain architecture for an intelligent decisi...  \n",
       "4       6581048  3-brain architecture for an intelligent decisi...  "
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(project_path + 'neural_network_patent_query.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dL2GIikd1WT2",
    "outputId": "43278bbb-c83f-4405-e670-b351ca53935b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3522, 4)"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "klvQnLT6zuYh"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(dataset, parse_dates=['patent_date']).dropna(subset = ['patent_abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jAQ027Ie1ROz",
    "outputId": "0033ddde-9876-43e5-91a1-66773d096bf2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3522, 4)"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lHpBR104ehuQ"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def format_sequence(s):\n",
    "    \"\"\"Add spaces around punctuation and remove references to images/citations.\"\"\"\n",
    "    \n",
    "    # Add spaces around punctuation\n",
    "    s =  re.sub(r'(?<=[^\\s0-9])(?=[.,;?])', r' ', s)\n",
    "    \n",
    "    # Remove references to figures\n",
    "    s = re.sub(r'\\((\\d+)\\)', r'', s)\n",
    "    \n",
    "    # Remove double spaces\n",
    "    s = re.sub(r'\\s\\s', ' ', s)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YPprnlH9zuao"
   },
   "outputs": [],
   "source": [
    "abstracts = [format_sequence(a) for a in list(data['patent_abstract'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "229NevhWz4y3",
    "outputId": "97b31e22-5c99-4cf2-ce9a-9178a56107d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3522"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xUQ_w7Y72CPj"
   },
   "source": [
    "### Tokenize the text\n",
    "\n",
    "Use tokenizer.fit_on_texts(`<list of texts>`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BCa_WYQV2Bff"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(lower=True, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(abstracts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fLj131WV13dH"
   },
   "outputs": [],
   "source": [
    "word_idx = tokenizer.word_index\n",
    "idx_word = tokenizer.index_word\n",
    "word_counts = tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TVvWTUda2ZEt",
    "outputId": "6b5d7bf2-783f-488c-9464-e1cae2b95e4a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11694"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx_word.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PvP29Cny2ppc"
   },
   "source": [
    "### Total no.of words in given dataset acc. to Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rrgm73wh2aV6"
   },
   "outputs": [],
   "source": [
    "num_words = len(word_idx) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "whNG30Z72xxV",
    "outputId": "d6d072da-c59a-4f15-aeb6-3185304a2eda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no.of words = 11695\n"
     ]
    }
   ],
   "source": [
    "print (\"Total no.of words = %d\" %(num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vl_34lMxgT1D"
   },
   "outputs": [],
   "source": [
    "num_words = 16192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zBaOPSbT2-PB"
   },
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "momCPZRs42Nf"
   },
   "source": [
    "### Consider only abstracts greater than 70 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zc2FD6Rd3BAE"
   },
   "outputs": [],
   "source": [
    "# Limit to sequences with more than training length tokens\n",
    "seq_lengths = [len(x) for x in sequences]\n",
    "over_idx = [i for i, l in enumerate(seq_lengths) if l > 70]\n",
    "\n",
    "new_texts = []\n",
    "new_sequences = []\n",
    "\n",
    "# Only keep sequences with more than training length tokens\n",
    "for i in over_idx:\n",
    "    new_texts.append(abstracts[i])\n",
    "    new_sequences.append(sequences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5_fK1CVs6VaO"
   },
   "source": [
    "### Generate features and labels\n",
    "\n",
    "If trainining_length is 50, take every 50 sequence as feature and every next word of each 50 sequence as label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CE33RLNW4emM"
   },
   "outputs": [],
   "source": [
    "features = []\n",
    "labels = []\n",
    "\n",
    "training_length = 50\n",
    "# Iterate through the sequences of tokens\n",
    "for seq in new_sequences:\n",
    "    \n",
    "    # Create multiple training examples from each sequence\n",
    "    for i in range(training_length, len(seq)):\n",
    "        # Extract the features and label\n",
    "        extract = seq[i - training_length: i + 1]\n",
    "        \n",
    "        # Set the features and label\n",
    "        features.append(extract[:-1])\n",
    "        labels.append(extract[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fHfVMlJU5Vml",
    "outputId": "ba49eea5-95ce-4bd0-c10b-b7ec9a0be819"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 293001 sequences.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are %d sequences.\" %(len(features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FIEpWlUN7BX_"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "\n",
    "features, labels = shuffle(features, labels, random_state=1)\n",
    "\n",
    "# Decide on number of samples for training\n",
    "train_end = int(0.7 * len(labels))\n",
    "\n",
    "train_features = np.array(features[:train_end])\n",
    "valid_features = np.array(features[train_end:])\n",
    "\n",
    "train_labels = labels[:train_end]\n",
    "valid_labels = labels[train_end:]\n",
    "\n",
    "# Convert to arrays\n",
    "X_train, X_valid = np.array(train_features), np.array(valid_features)\n",
    "\n",
    "# Using int8 for memory savings\n",
    "y_train = np.zeros((len(train_labels), num_words), dtype=np.int8)\n",
    "y_valid = np.zeros((len(valid_labels), num_words), dtype=np.int8)\n",
    "\n",
    "# One hot encoding of labels\n",
    "for example_index, word_index in enumerate(train_labels):\n",
    "    y_train[example_index, word_index] = 1\n",
    "\n",
    "for example_index, word_index in enumerate(valid_labels):\n",
    "    y_valid[example_index, word_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "siGq7-2y8E0P",
    "outputId": "846d85d7-c78d-40fd-922c-437e5cf91952"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: creates a digital representation of the arc created during welding and using a neural network computer determines if the arc is representative of normal or abnormal welding conditions the neural network disclosed is trained to identify abnormal conditions and normal conditions and may be adaptively retrained to classify images that\n",
      "\n",
      "Label: are\n",
      "\n",
      "Features: data mining case based reasoning rule based reasoning fuzzy logic constraint programming and genetic algorithms the systems and methods of the present invention involve a fraud detection and prevention model that successfully detects and prevents electronic fraud and network intrusion in real time the model is not sensitive to known\n",
      "\n",
      "Label: or\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, sequence in enumerate(X_train[:2]):\n",
    "    text = []\n",
    "    for idx in sequence:\n",
    "        text.append(idx_word[idx])\n",
    "        \n",
    "    print('Features: ' + ' '.join(text) + '\\n')\n",
    "    print('Label: ' + idx_word[np.argmax(y_train[i])] + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uRk5Ocn58x0f"
   },
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X3oD0n_m82ZQ"
   },
   "source": [
    "Embedding dimension = 100\n",
    "\n",
    "64 LSTM cells in one layer\n",
    "\n",
    "Dropout and recurrent dropout for regularization\n",
    "\n",
    "Fully connected layer with 64 units on top of LSTM\n",
    "\n",
    "'relu' activation\n",
    "\n",
    "Drop out for regularization\n",
    "\n",
    "Output layer produces prediction for each word\n",
    "\n",
    "'softmax' activation\n",
    "\n",
    "Adam optimizer with defaults\n",
    "\n",
    "Categorical cross entropy loss\n",
    "\n",
    "Monitor accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XmQH-uLt8sNe"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "SaDoW9ew9JP-",
    "outputId": "2d3b689a-9646-465c-fe1d-a684d9d73c5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 100)         1169500   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16192)             1052480   \n",
      "=================================================================\n",
      "Total params: 2,268,380\n",
      "Trainable params: 2,268,380\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model.add(\n",
    "    Embedding(\n",
    "        input_dim=len(word_idx) + 1,\n",
    "        output_dim=100,\n",
    "        weights=None,\n",
    "        trainable=True))\n",
    "\n",
    "# Recurrent layer\n",
    "model.add(\n",
    "    LSTM(\n",
    "        64, return_sequences=False))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Dropout for regularization\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(num_words, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "05uza6tHA_hq"
   },
   "source": [
    "<!-- ### Training -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wMKbYhWO-EtV"
   },
   "source": [
    "### Load in Pre-Trained Model\n",
    "Rather than waiting several hours to train the model, we can load in a model trained for 150 epochs. We'll demonstrate how to train this model for another 5 epochs which shouldn't take too long depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "colab_type": "code",
    "id": "G3-Nb2bM9rvR",
    "outputId": "f1b84cf5-8622-4f6a-e2a8-e53ef08962b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 205100 samples, validate on 87901 samples\n",
      "Epoch 1/20\n",
      "205100/205100 [==============================] - 41s 200us/step - loss: 7.1118 - acc: 0.0928 - val_loss: 6.3357 - val_acc: 0.1094\n",
      "Epoch 2/20\n",
      "205100/205100 [==============================] - 40s 194us/step - loss: 6.4278 - acc: 0.1081 - val_loss: 6.1811 - val_acc: 0.1137\n",
      "Epoch 3/20\n",
      "205100/205100 [==============================] - 40s 193us/step - loss: 6.2771 - acc: 0.1157 - val_loss: 6.0834 - val_acc: 0.1250\n",
      "Epoch 4/20\n",
      "205100/205100 [==============================] - 40s 194us/step - loss: 6.1690 - acc: 0.1235 - val_loss: 5.9972 - val_acc: 0.1317\n",
      "Epoch 5/20\n",
      "205100/205100 [==============================] - 40s 195us/step - loss: 6.0810 - acc: 0.1313 - val_loss: 5.9271 - val_acc: 0.1418\n",
      "Epoch 6/20\n",
      "205100/205100 [==============================] - 40s 194us/step - loss: 5.9994 - acc: 0.1372 - val_loss: 5.8569 - val_acc: 0.1485\n",
      "Epoch 7/20\n",
      "205100/205100 [==============================] - 40s 195us/step - loss: 5.9280 - acc: 0.1432 - val_loss: 5.8014 - val_acc: 0.1557\n",
      "Epoch 8/20\n",
      "205100/205100 [==============================] - 40s 196us/step - loss: 5.8646 - acc: 0.1488 - val_loss: 5.7500 - val_acc: 0.1599\n",
      "Epoch 9/20\n",
      "205100/205100 [==============================] - 40s 196us/step - loss: 5.8060 - acc: 0.1533 - val_loss: 5.7065 - val_acc: 0.1633\n",
      "Epoch 10/20\n",
      "205100/205100 [==============================] - 40s 196us/step - loss: 5.7542 - acc: 0.1564 - val_loss: 5.6686 - val_acc: 0.1663\n",
      "Epoch 11/20\n",
      "205100/205100 [==============================] - 40s 195us/step - loss: 5.7068 - acc: 0.1597 - val_loss: 5.6323 - val_acc: 0.1693\n",
      "Epoch 12/20\n",
      "205100/205100 [==============================] - 40s 197us/step - loss: 5.6612 - acc: 0.1621 - val_loss: 5.6017 - val_acc: 0.1706\n",
      "Epoch 13/20\n",
      "205100/205100 [==============================] - 40s 197us/step - loss: 5.6206 - acc: 0.1640 - val_loss: 5.5731 - val_acc: 0.1734\n",
      "Epoch 14/20\n",
      "205100/205100 [==============================] - 40s 196us/step - loss: 5.5807 - acc: 0.1667 - val_loss: 5.5483 - val_acc: 0.1743\n",
      "Epoch 15/20\n",
      "205100/205100 [==============================] - 40s 196us/step - loss: 5.5449 - acc: 0.1679 - val_loss: 5.5264 - val_acc: 0.1760\n",
      "Epoch 16/20\n",
      "205100/205100 [==============================] - 40s 196us/step - loss: 5.5104 - acc: 0.1704 - val_loss: 5.5044 - val_acc: 0.1771\n",
      "Epoch 17/20\n",
      "205100/205100 [==============================] - 40s 197us/step - loss: 5.4773 - acc: 0.1716 - val_loss: 5.4827 - val_acc: 0.1786\n",
      "Epoch 18/20\n",
      "205100/205100 [==============================] - 40s 197us/step - loss: 5.4482 - acc: 0.1731 - val_loss: 5.4637 - val_acc: 0.1796\n",
      "Epoch 19/20\n",
      "205100/205100 [==============================] - 40s 195us/step - loss: 5.4134 - acc: 0.1750 - val_loss: 5.4421 - val_acc: 0.1817\n",
      "Epoch 20/20\n",
      "205100/205100 [==============================] - 40s 196us/step - loss: 5.3869 - acc: 0.1756 - val_loss: 5.4246 - val_acc: 0.1828\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Load in model and demonstrate training\n",
    "model = load_model(project_path + 'train-embeddings-rnn.h5')\n",
    "h = model.fit(X_train, y_train, epochs = 20, batch_size = 2048, \n",
    "          validation_data = (X_valid, y_valid), \n",
    "          verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y1sX3C4h_LSW"
   },
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "WLul448R_Ms9",
    "outputId": "94c32dbd-b1b7-45fe-c653-d8c2703f9c43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205100/205100 [==============================] - 18s 89us/step\n",
      "[5.122475255336603, 0.18782057532271426]\n",
      "\n",
      "Model Performance: Log Loss and Accuracy on validation data\n",
      "87901/87901 [==============================] - 8s 89us/step\n",
      "[5.424596187795825, 0.1827624257987444]\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(X_train, y_train, batch_size = 2048))\n",
    "print('\\nModel Performance: Log Loss and Accuracy on validation data')\n",
    "print(model.evaluate(X_valid, y_valid, batch_size = 2048))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4hXd0HbuAzfw"
   },
   "source": [
    "### Generate Text \n",
    "\n",
    "Run this to check the text output by the model. This function randomly generates input of length 50 words for the model and then generates the next 50 words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "bOjFFnmuAHTz",
    "outputId": "1a6da40f-724a-4b42-87cc-025911cc9f5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "received for the person to person communication program the communication is processed to determine predefined language statements information on the determined language statements is inputted into a neural network to produce an output value a determination is made as to whether the output value indicates that the communication is unacceptable\n",
      "\n",
      "\n",
      "differences of a signal circuit and providing convolution variables by each node of a predetermined control section based for a load strings for supplying and another database a target probability the overall process units for or dividing one or more characteristics corresponding to activation filtering values in the invention is\n"
     ]
    }
   ],
   "source": [
    "seed_length=50\n",
    "new_words=50\n",
    "diversity=1\n",
    "n_gen=1\n",
    "\n",
    "import random\n",
    "\n",
    "# Choose a random sequence\n",
    "seq = random.choice(sequences)\n",
    "\n",
    "# Choose a random starting point\n",
    "seed_idx = random.randint(0, len(seq) - seed_length - 10)\n",
    "# Ending index for seed\n",
    "end_idx = seed_idx + seed_length\n",
    "\n",
    "gen_list = []\n",
    "\n",
    "for n in range(n_gen):\n",
    "    # Extract the seed sequence\n",
    "    seed = seq[seed_idx:end_idx]\n",
    "    original_sequence = [idx_word[i] for i in seed]\n",
    "    generated = seed[:] + ['#']\n",
    "\n",
    "    # Find the actual entire sequence\n",
    "    actual = generated[:] + seq[end_idx:end_idx + new_words]\n",
    "        \n",
    "    # Keep adding new words\n",
    "    for i in range(new_words):\n",
    "\n",
    "        # Make a prediction from the seed\n",
    "        preds = model.predict(np.array(seed).reshape(1, -1))[0].astype(np.float64)\n",
    "\n",
    "        # Diversify\n",
    "        preds = np.log(preds) / diversity\n",
    "        exp_preds = np.exp(preds)\n",
    "\n",
    "        # Softmax\n",
    "        preds = exp_preds / sum(exp_preds)\n",
    "\n",
    "        # Choose the next word\n",
    "        probas = np.random.multinomial(1, preds, 1)[0]\n",
    "\n",
    "        next_idx = np.argmax(probas)\n",
    "\n",
    "        # New seed adds on old word\n",
    "        #             seed = seed[1:] + [next_idx]\n",
    "        seed += [next_idx]\n",
    "        generated.append(next_idx)\n",
    "    # Showing generated and actual abstract\n",
    "    n = []\n",
    "\n",
    "    for i in generated:\n",
    "        n.append(idx_word.get(i, '< --- >'))\n",
    "\n",
    "    gen_list.append(n)\n",
    "\n",
    "a = []\n",
    "\n",
    "for i in actual:\n",
    "    a.append(idx_word.get(i, '< --- >'))\n",
    "\n",
    "a = a[seed_length:]\n",
    "\n",
    "gen_list = [gen[seed_length:seed_length + len(a)] for gen in gen_list]\n",
    "\n",
    "print ' '.join(original_sequence)\n",
    "print \"\\n\"\n",
    "# print gen_list\n",
    "print ' '.join(gen_list[0][1:])\n",
    "# print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dz6e2ZsOjiV0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TextGeneration_Lab_Solution.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
