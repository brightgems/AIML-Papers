{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QambAoV-BNmI"
   },
   "source": [
    "## Text Generation Using patent abstracts from patent search for `neural network`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FM4VYS13uUq9"
   },
   "source": [
    "### Files required:\n",
    "\n",
    "1. `neural_network_patent_query.csv`\n",
    "\n",
    "2. `train-embeddings.h5`\n",
    "\n",
    "Copy the above files to your drive from [this](https://drive.google.com/drive/folders/1cbAesB-eejsRKdCHpnFSyXiu81Y5a5HU?usp=sharing) link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0s5ciFSXzeWb"
   },
   "source": [
    "### Read the dataset \n",
    "\n",
    "Use variable name for the dataframe as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wdLKlh1N0fkr"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "w9_uC5oJ0fza",
    "outputId": "57e5934f-7ca5-4610-8a24-efd69fb88c03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZAiZ0yAp0f-Z"
   },
   "outputs": [],
   "source": [
    "path = \"/content/drive/My Drive/AIML/Lab/SequenceModels/Internal/\"\n",
    "file = \"neural_network_patent_query.csv\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(path + file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hBddk1Lq0p72"
   },
   "source": [
    "### Check 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "BCheYc6azdFl",
    "outputId": "1cadbde7-ce64-464b-bd28-34b8f765f84c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patent_abstract</th>\n",
       "      <th>patent_date</th>\n",
       "      <th>patent_number</th>\n",
       "      <th>patent_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\" A \"\"Barometer\"\" Neuron enhances stability in...</td>\n",
       "      <td>1996-07-09</td>\n",
       "      <td>5535303</td>\n",
       "      <td>\"\"\"Barometer\"\" neuron for a neural network\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\" This invention is a novel high-speed neural ...</td>\n",
       "      <td>1993-10-19</td>\n",
       "      <td>5255349</td>\n",
       "      <td>\"Electronic neural network for solving \"\"trave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An optical information processor for use as a ...</td>\n",
       "      <td>1995-01-17</td>\n",
       "      <td>5383042</td>\n",
       "      <td>3 layer liquid crystal neural network with out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A method and system for intelligent control of...</td>\n",
       "      <td>2001-01-02</td>\n",
       "      <td>6169981</td>\n",
       "      <td>3-brain architecture for an intelligent decisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A method and system for intelligent control of...</td>\n",
       "      <td>2003-06-17</td>\n",
       "      <td>6581048</td>\n",
       "      <td>3-brain architecture for an intelligent decisi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     patent_abstract patent_date  \\\n",
       "0  \" A \"\"Barometer\"\" Neuron enhances stability in...  1996-07-09   \n",
       "1  \" This invention is a novel high-speed neural ...  1993-10-19   \n",
       "2  An optical information processor for use as a ...  1995-01-17   \n",
       "3  A method and system for intelligent control of...  2001-01-02   \n",
       "4  A method and system for intelligent control of...  2003-06-17   \n",
       "\n",
       "  patent_number                                       patent_title  \n",
       "0       5535303        \"\"\"Barometer\"\" neuron for a neural network\"  \n",
       "1       5255349  \"Electronic neural network for solving \"\"trave...  \n",
       "2       5383042  3 layer liquid crystal neural network with out...  \n",
       "3       6169981  3-brain architecture for an intelligent decisi...  \n",
       "4       6581048  3-brain architecture for an intelligent decisi...  "
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dL2GIikd1WT2",
    "outputId": "37937772-43e8-482b-9d27-93499b1182a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3522, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tZUhOdGh0y3R"
   },
   "source": [
    "Now, all the patent abstract data is in `data['patent_abstract']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "naiqlFGD1Jkg"
   },
   "source": [
    "For ease of access, assign a variable name `abstracts` to `data['patent_abstract']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E6JnwEsg1JY1"
   },
   "outputs": [],
   "source": [
    "abstracts = data['patent_abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Hk7o1Py_ZrZ7",
    "outputId": "fc670aeb-6a5d-41db-9286-ef2e1a8d97a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3522,)\n"
     ]
    }
   ],
   "source": [
    "print(abstracts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xUQ_w7Y72CPj"
   },
   "source": [
    "### Tokenize the text\n",
    "\n",
    "Initialize the Tokenizer class with variable name `tokenizer`\n",
    "\n",
    "Use tokenizer.fit_on_texts(`<list of texts>`) on `abstracts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "E4HEMGZk13hl",
    "outputId": "94e232e8-8c08-411d-aa0f-9c510e45adaf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(lower=True)\n",
    "tokenizer.fit_on_texts(abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "awXmxPRE1vWI"
   },
   "source": [
    "### Run the below code to extract insights from tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fLj131WV13dH"
   },
   "outputs": [],
   "source": [
    "word_idx = tokenizer.word_index\n",
    "idx_word = tokenizer.index_word\n",
    "word_counts = tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xeGVwGQtaalH"
   },
   "outputs": [],
   "source": [
    "#print(word_idx)\n",
    "#print(idx_word)\n",
    "#print(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PvP29Cny2ppc"
   },
   "source": [
    "### Total no.of words in given dataset acc. to Tokenizer\n",
    "\n",
    "Run the below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rrgm73wh2aV6"
   },
   "outputs": [],
   "source": [
    "num_words = len(word_idx) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "whNG30Z72xxV",
    "outputId": "f9453ada-b6ea-4eae-e899-ce0ee9f8be93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no.of words = 11755\n"
     ]
    }
   ],
   "source": [
    "print (\"Total no.of words = %d\" %(num_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "etKjgOIC19Ws"
   },
   "source": [
    "### The given pre-trained model `train-embeddings.h5` is on 16192 tokens, hence take num_words as 16192 to use the pre-trained model.\n",
    "\n",
    "Run the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vl_34lMxgT1D"
   },
   "outputs": [],
   "source": [
    "num_words = 16192"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JhIA8mM42kTV"
   },
   "source": [
    "### Encode words to integers using texts_to_sequences in keras\n",
    "\n",
    "Use variable name `sequences`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5juI1fJP2xA2"
   },
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SXUXIHg6beRF"
   },
   "outputs": [],
   "source": [
    "#print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "momCPZRs42Nf"
   },
   "source": [
    "### Consider only abstracts greater than 70 words\n",
    "\n",
    "Run the below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zc2FD6Rd3BAE"
   },
   "outputs": [],
   "source": [
    "seq_lengths = [len(x) for x in sequences]\n",
    "over_idx = [i for i, l in enumerate(seq_lengths) if l > 70]\n",
    "\n",
    "new_texts = []\n",
    "new_sequences = []\n",
    "\n",
    "# Only keep sequences with more than training length tokens\n",
    "for i in over_idx:\n",
    "    new_texts.append(abstracts[i])\n",
    "    new_sequences.append(sequences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bKptlkmqc2-k",
    "outputId": "4afff607-9083-4078-8300-aeb90718a041"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3195\n"
     ]
    }
   ],
   "source": [
    "print(len(new_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qBCJ6t3Kc3B8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RbTQZxHQ3XMH"
   },
   "source": [
    "Now, we have abstracts in new_texts and words encoded to integers in new_sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5_fK1CVs6VaO"
   },
   "source": [
    "### Generate features and labels\n",
    "\n",
    "If trainining_length is 50, take every 49 sequence as feature and every last word of each 50 sequence as label.\n",
    "\n",
    "Run the below code to generate features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CE33RLNW4emM"
   },
   "outputs": [],
   "source": [
    "features = []\n",
    "labels = []\n",
    "\n",
    "training_length = 50\n",
    "# Iterate through the sequences of tokens\n",
    "for each_sequence in new_sequences:\n",
    "    \n",
    "    # Create multiple training examples from each sequence\n",
    "    for i in range(training_length, len(each_sequence)):\n",
    "        # Extract the features and label\n",
    "        extract = each_sequence[i - training_length: i + 1]\n",
    "        \n",
    "        # Set the features and label\n",
    "        features.append(extract[:-1])\n",
    "        labels.append(extract[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "fHfVMlJU5Vml",
    "outputId": "fa61b865-fd18-4319-9313-538f5874dd29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 294130 sequences.\n",
      "There are 294130 labels.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are %d sequences.\" %(len(features)))\n",
    "print(\"There are %d labels.\" %(len(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-3GWbIKhzhy"
   },
   "outputs": [],
   "source": [
    "#print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-qfLUN1SjeLw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cSc7P2zM4fYt"
   },
   "source": [
    "### Split into train and validation sets\n",
    "\n",
    "1. Shuffle the features and labels accordingly.\n",
    "\n",
    "2. Split into train and validation sets. Use variable names X_train, X_valid, y_train and y_valid accordingly. Consider 70:30\n",
    "\n",
    "3. Convert y_train and y_valid to one-hot encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FIEpWlUN7BX_"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fuPO7ZcAocz6"
   },
   "outputs": [],
   "source": [
    "features, labels = shuffle(features, labels, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZJmRjRUn58TN"
   },
   "outputs": [],
   "source": [
    "#X_train, X_valid, y_train, y_valid = train_test_split(features, labels, test_size=0.3, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eIRajkCL58Vx"
   },
   "outputs": [],
   "source": [
    "#print(len(X_train), len(X_valid), len(y_train), len(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EaoElbfkj4bD"
   },
   "outputs": [],
   "source": [
    "#from keras.utils import to_categorical\n",
    "#y_train = to_categorical(y_train)\n",
    "#y_valid = to_categorical(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m33IPOFGpDZu"
   },
   "outputs": [],
   "source": [
    "# Decide on number of samples for training\n",
    "train_end = int(0.7 * len(labels))\n",
    "\n",
    "train_features = np.array(features[:train_end])\n",
    "valid_features = np.array(features[train_end:])\n",
    "\n",
    "train_labels = labels[:train_end]\n",
    "valid_labels = labels[train_end:]\n",
    "\n",
    "# Convert to arrays\n",
    "X_train, X_valid = np.array(train_features), np.array(valid_features)\n",
    "\n",
    "# Using int8 for memory savings\n",
    "y_train = np.zeros((len(train_labels), num_words), dtype=np.int8)\n",
    "y_valid = np.zeros((len(valid_labels), num_words), dtype=np.int8)\n",
    "\n",
    "# One hot encoding of labels\n",
    "for example_index, word_index in enumerate(train_labels):\n",
    "    y_train[example_index, word_index] = 1\n",
    "\n",
    "for example_index, word_index in enumerate(valid_labels):\n",
    "    y_valid[example_index, word_index] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zxzyd04N5ItJ"
   },
   "source": [
    "### Check 2\n",
    "\n",
    "Run the below code to check some features and their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "colab_type": "code",
    "id": "siGq7-2y8E0P",
    "outputId": "7a6c5003-32af-4377-c2cb-93b3dd98261e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: a memory unit 40 for storing data an arithmetic unit 42 for mathematically operating on the data a memory address generation unit 32 and an adder for computing a next memory address the memory address generation unit 32 includes an address register 34 in the memory unit for identifying the\n",
      "\n",
      "Label: address\n",
      "\n",
      "Features: to improve the perceptual quality of the speech and background noise under a variety of input conditions the present invention also improves the voicing dependent spectral estimation algorithm robustness by introducing the use of a multi layer neural network in the estimation process the voicing dependent spectral estimation algorithm provides\n",
      "\n",
      "Label: an\n",
      "\n",
      "Features: and computational efficiency to allow the mann to be applied to full digital images without operator input the hybrid filter architecture and mann may be applied to any gray scale image in medical imaging the specific application of the proposed method includes a improved enhancement or detection of suspicious areas\n",
      "\n",
      "Label: as\n",
      "\n",
      "Features: combinations or pairs on the basis of the extracted result of the acoustic feature intrinsic to each of said candidate speeches and a final decision unit for collecting all the pair discrimination results obtained from the pair discrimination unit for each of all the sub n c sub 2 number\n",
      "\n",
      "Label: of\n",
      "\n",
      "Features: text in a subsequently received image and may do so automatically and without user intervention e g without specifying any of the edges of a bounding box in a second example a deep neural network is directly learned as an embedding function of a model that is usable to determine\n",
      "\n",
      "Label: font\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, sequence in enumerate(X_train[:5]):\n",
    "    text = []\n",
    "    for idx in sequence:\n",
    "        text.append(idx_word[idx])\n",
    "        \n",
    "    print('Features: ' + ' '.join(text) + '\\n')\n",
    "    print('Label: ' + idx_word[np.argmax(y_train[i])] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uRk5Ocn58x0f"
   },
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X3oD0n_m82ZQ"
   },
   "source": [
    "#### Consider the following details while building the model.\n",
    "\n",
    "Embedding dimension = 100\n",
    "\n",
    "64 LSTM cells in one layer with return_sequences as `False`\n",
    "\n",
    "Fully connected layer with 64 units on top of LSTM\n",
    "\n",
    "'relu' activation\n",
    "\n",
    "Drop out for regularization\n",
    "\n",
    "Output Dense layer with size of num_words for matching the size of one-hot encoding of each word\n",
    "\n",
    "'softmax' activation\n",
    "\n",
    "Categorical cross entropy loss\n",
    "\n",
    "Metric accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i_Rz6YIQ6nrj"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RbinXsAO6nuF"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model.add(\n",
    "    Embedding(\n",
    "        input_dim=len(word_idx) + 1,\n",
    "        output_dim=100,\n",
    "        weights=None,\n",
    "        trainable=True))\n",
    "\n",
    "# Recurrent layer\n",
    "model.add(\n",
    "    LSTM(\n",
    "        64, return_sequences=False, dropout=0.1,\n",
    "        recurrent_dropout=0.1))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Dropout for regularization\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(num_words, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "id": "wfMC-pqJ6nwr",
    "outputId": "167da1f8-6f15-4595-cb0d-56e2fa746394"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         1175500   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16192)             1052480   \n",
      "=================================================================\n",
      "Total params: 2,274,380\n",
      "Trainable params: 2,274,380\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "05uza6tHA_hq"
   },
   "source": [
    "<!-- ### Training -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wMKbYhWO-EtV"
   },
   "source": [
    "### Load in Pre-Trained Model\n",
    "We can load in a model trained for 150 epochs and train this model for another 20 epochs.\n",
    "\n",
    "1. Import `load_model` from `keras.models`\n",
    "\n",
    "2. Load the model file `train-embeddings.h5` using `load_model`. Use variable name `model`.\n",
    "\n",
    "3. Do model.fit() on training and validation sets to train the model. Consider batch_size as 2048 and epochs as 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eon_nPGt65ak"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 706
    },
    "colab_type": "code",
    "id": "G3-Nb2bM9rvR",
    "outputId": "ccab3757-b394-4193-cbdf-3b23dc9aedea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 205891 samples, validate on 88239 samples\n",
      "Epoch 1/20\n",
      "205891/205891 [==============================] - 40s 192us/step - loss: 7.1215 - acc: 0.0923 - val_loss: 6.3456 - val_acc: 0.1070\n",
      "Epoch 2/20\n",
      "205891/205891 [==============================] - 38s 183us/step - loss: 6.4472 - acc: 0.1072 - val_loss: 6.1979 - val_acc: 0.1112\n",
      "Epoch 3/20\n",
      "205891/205891 [==============================] - 38s 182us/step - loss: 6.3029 - acc: 0.1138 - val_loss: 6.1044 - val_acc: 0.1238\n",
      "Epoch 4/20\n",
      "205891/205891 [==============================] - 38s 182us/step - loss: 6.1948 - acc: 0.1215 - val_loss: 6.0227 - val_acc: 0.1306\n",
      "Epoch 5/20\n",
      "205891/205891 [==============================] - 38s 182us/step - loss: 6.1096 - acc: 0.1280 - val_loss: 5.9531 - val_acc: 0.1392\n",
      "Epoch 6/20\n",
      "205891/205891 [==============================] - 37s 182us/step - loss: 6.0346 - acc: 0.1340 - val_loss: 5.8896 - val_acc: 0.1445\n",
      "Epoch 7/20\n",
      "205891/205891 [==============================] - 38s 182us/step - loss: 5.9610 - acc: 0.1392 - val_loss: 5.8341 - val_acc: 0.1493\n",
      "Epoch 8/20\n",
      "205891/205891 [==============================] - 37s 182us/step - loss: 5.9014 - acc: 0.1437 - val_loss: 5.7860 - val_acc: 0.1523\n",
      "Epoch 9/20\n",
      "205891/205891 [==============================] - 37s 181us/step - loss: 5.8408 - acc: 0.1473 - val_loss: 5.7403 - val_acc: 0.1592\n",
      "Epoch 10/20\n",
      "205891/205891 [==============================] - 37s 182us/step - loss: 5.7827 - acc: 0.1523 - val_loss: 5.7011 - val_acc: 0.1625\n",
      "Epoch 11/20\n",
      "205891/205891 [==============================] - 37s 182us/step - loss: 5.7364 - acc: 0.1550 - val_loss: 5.6624 - val_acc: 0.1661\n",
      "Epoch 12/20\n",
      "205891/205891 [==============================] - 38s 182us/step - loss: 5.6908 - acc: 0.1587 - val_loss: 5.6328 - val_acc: 0.1674\n",
      "Epoch 13/20\n",
      "205891/205891 [==============================] - 38s 182us/step - loss: 5.6481 - acc: 0.1606 - val_loss: 5.5991 - val_acc: 0.1700\n",
      "Epoch 14/20\n",
      "205891/205891 [==============================] - 38s 182us/step - loss: 5.6050 - acc: 0.1633 - val_loss: 5.5721 - val_acc: 0.1719\n",
      "Epoch 15/20\n",
      "205891/205891 [==============================] - 37s 182us/step - loss: 5.5679 - acc: 0.1651 - val_loss: 5.5475 - val_acc: 0.1731\n",
      "Epoch 16/20\n",
      "205891/205891 [==============================] - 39s 188us/step - loss: 5.5316 - acc: 0.1676 - val_loss: 5.5247 - val_acc: 0.1751\n",
      "Epoch 17/20\n",
      "205891/205891 [==============================] - 39s 189us/step - loss: 5.4966 - acc: 0.1700 - val_loss: 5.5039 - val_acc: 0.1766\n",
      "Epoch 18/20\n",
      "205891/205891 [==============================] - 37s 182us/step - loss: 5.4632 - acc: 0.1724 - val_loss: 5.4818 - val_acc: 0.1801\n",
      "Epoch 19/20\n",
      "205891/205891 [==============================] - 37s 182us/step - loss: 5.4330 - acc: 0.1737 - val_loss: 5.4635 - val_acc: 0.1820\n",
      "Epoch 20/20\n",
      "205891/205891 [==============================] - 37s 182us/step - loss: 5.4006 - acc: 0.1756 - val_loss: 5.4460 - val_acc: 0.1837\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Load in model and demonstrate training\n",
    "model = load_model(path + 'train-embeddings.h5')\n",
    "h = model.fit(X_train, y_train, epochs = 20, batch_size = 2048, \n",
    "          validation_data = (X_valid, y_valid), \n",
    "          verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4hXd0HbuAzfw"
   },
   "source": [
    "### Generate Text \n",
    "\n",
    "Run this to check the text output by the model. This function randomly generates input of length 50 words for the model and then generates the next 50 words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "bOjFFnmuAHTz",
    "outputId": "722c0acc-e758-429e-9f67-29133fa01a21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reliability or validity of the output as a function of the number of data points in a given data region during training of the system model 12 this predicts the confidence in the predicted output which is also input to the decision processor 20 the decision processor 20 therefore bases\n",
      "\n",
      "\n",
      "applied in accordance with the battery module by such that out for in an other audio process image which is controlling the neuron on the number parameters to the\n"
     ]
    }
   ],
   "source": [
    "seed_length=50\n",
    "new_words=50\n",
    "diversity=1\n",
    "n_gen=1\n",
    "\n",
    "import random\n",
    "\n",
    "# Choose a random sequence\n",
    "seq = random.choice(sequences)\n",
    "\n",
    "# Choose a random starting point\n",
    "seed_idx = random.randint(0, len(seq) - seed_length - 10)\n",
    "# Ending index for seed\n",
    "end_idx = seed_idx + seed_length\n",
    "\n",
    "gen_list = []\n",
    "\n",
    "for n in range(n_gen):\n",
    "    # Extract the seed sequence\n",
    "    seed = seq[seed_idx:end_idx]\n",
    "    original_sequence = [idx_word[i] for i in seed]\n",
    "    generated = seed[:] + ['#']\n",
    "\n",
    "    # Find the actual entire sequence\n",
    "    actual = generated[:] + seq[end_idx:end_idx + new_words]\n",
    "        \n",
    "    # Keep adding new words\n",
    "    for i in range(new_words):\n",
    "\n",
    "        # Make a prediction from the seed\n",
    "        preds = model.predict(np.array(seed).reshape(1, -1))[0].astype(np.float64)\n",
    "\n",
    "        # Diversify\n",
    "        preds = np.log(preds) / diversity\n",
    "        exp_preds = np.exp(preds)\n",
    "\n",
    "        # Softmax\n",
    "        preds = exp_preds / sum(exp_preds)\n",
    "\n",
    "        # Choose the next word\n",
    "        probas = np.random.multinomial(1, preds, 1)[0]\n",
    "\n",
    "        next_idx = np.argmax(probas)\n",
    "\n",
    "        # New seed adds on old word\n",
    "        #             seed = seed[1:] + [next_idx]\n",
    "        seed += [next_idx]\n",
    "        generated.append(next_idx)\n",
    "    # Showing generated and actual abstract\n",
    "    n = []\n",
    "\n",
    "    for i in generated:\n",
    "        n.append(idx_word.get(i, '< --- >'))\n",
    "\n",
    "    gen_list.append(n)\n",
    "\n",
    "a = []\n",
    "\n",
    "for i in actual:\n",
    "    a.append(idx_word.get(i, '< --- >'))\n",
    "\n",
    "a = a[seed_length:]\n",
    "\n",
    "gen_list = [gen[seed_length:seed_length + len(a)] for gen in gen_list]\n",
    "\n",
    "print ' '.join(original_sequence)\n",
    "print \"\\n\"\n",
    "# print gen_list\n",
    "print ' '.join(gen_list[0][1:])\n",
    "# print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dz6e2ZsOjiV0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TextGeneration_Lab_Questions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
